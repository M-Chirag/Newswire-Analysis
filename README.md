# Newswire-Analysis
![image](https://user-images.githubusercontent.com/43926813/207499689-4f4f0ee7-3563-46eb-9b94-9b117abfc1a9.png)


Analyzing and Classifying Newswires to tackle misinformation and fake news

The objective of my project is to develop and use algorithms that can automatically group together news articles that have a similar meaning after being educated on collections of texts from newswires in a dataset.
I intend to use the concepts we learnt in class (INFO202) to build topic clusters and analyze the sentiments based on the word embeddings in the newswires. The concepts used here are categorization, tokenization, forming hierarchical structures, and semantic similarities with word embeddings. This can assist in directing us to develop a tidy and disciplined organization system for gathering and arranging massive amounts of text information. These word level embeddings, in my opinion, may be used to organize the topics in news articles to tackle the spread of misinformation. Additionally, I have applied the concept of grounded coding to assign codes (top 5 topics) to 40 documents from the dataset and compare them to the one generated by the LDA model using the Inter-Annotator Agreement.

# Implementation Steps:

1. Data Gathering: 
  I made use of the open source data available on Reuters-Dataset (https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection) where a large collection of news articles is maintained. The documents were assembled and indexed with categories by personnel from Reuters Ltd.

2. Preprocessing: 
  Data cleaning, Exploratory Data Analysis: Based on my preliminary idea by looking at the dataset, I will be conducting 1 and 2 level data cleaning for the acquired data, preprocessing to remove superfluous terms and suppress some words, altering various versions of the same word, and finally discarding words with illegal characters.

3. Bag of Words: I intend to use the BOW model,  by breaking down the newswires into terms with corresponding frequencies (often used words), after which I will visualize using word clouds and bar graphs by type of article (Opioid, Reviews, Editorials, etc.).

4. Word Embedding and Document Embedding
The idea behind word and document embedding is based on converting text into vectors while also taking the context into consideration. It works on the logic “you are the average of the people you spend time with”. The idea behind doc2vec is a context window that slides between the paragraphs while capturing the meaning of the words.

5. Analyzing clusters: I will analyze the distribution of the word in each cluster and overall news articles to relate it to our organization concepts from class.

6. Visualizing : I intend to use a relevant visualizing generator API pyLDAvis to simplify the visualization of any vector having many dimensions and convert it into human understandable visualizations.


# Visualizations and Interpretations


![Recording 2022-12-13 at 19 46 38](https://user-images.githubusercontent.com/43926813/207500628-775404e6-82ce-4051-9914-52dbb637d9be.gif)

![word_cloud](https://user-images.githubusercontent.com/43926813/207500179-d0c083f1-d9b3-437c-b287-b17d3d82039e.png)

![Topic_Percent](https://user-images.githubusercontent.com/43926813/207500230-86b9f10a-29ef-4092-91d8-4e4ee0f2d1ce.png)

![distribution_words](https://user-images.githubusercontent.com/43926813/207500239-87adea2e-bb74-4f4e-8508-85df1ade650a.png)
